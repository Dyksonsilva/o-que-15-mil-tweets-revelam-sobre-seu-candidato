{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise\n",
    "Script para limpar e categorizar a base de dados de tweets. Ao final, o programa salva arquivos `.csv` com os valores de frequência de uso e \"unicidade\" para cada palavra dos candidatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lê os arquivos\n",
    "files = glob.glob('../data/bases-completas/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cria um dataframe vazio e adiciona os arquivos de cada candidato\n",
    "columns = ['id', 'datetime', 'created_at', 'text', 'file', 'twitter_handle']\n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rs116474/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Junta todos os csvs em um único dataframe\n",
    "for file in files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    temp_df['file'] = file\n",
    "    temp_df['twitter_handle'] = temp_df.file.str.extract('../data/bases-completas/(\\w+).csv')\n",
    "    df = df.append(temp_df)\n",
    "    df = df.reset_index().drop('index', axis=1) # Cria índices únicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforma a coluna datetime em objeto temporal\n",
    "df.datetime = pd.to_datetime(df.datetime, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "# Mantém apenas os que não são RT\n",
    "df = df[~df.text.str.startswith(\"RT\")]\n",
    "# Pega apenas tweets a partir de 27-09-2017 - data a partir da qual temos tweets de todos os candidatos\n",
    "df = df[df.datetime >= pd.to_datetime(\"2017-09-28 00:00:00\", format=\"%Y-%m-%d %H:%M:%S\")]\n",
    "# Derrupa duplicatas retornadas eventualmente pela api do twitter\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Captura os nomes reais de cada candidato\n",
    "def get_names(row):\n",
    "    if row.twitter_handle == 'cirogomes':\n",
    "        name = 'Ciro Gomes'\n",
    "    if row.twitter_handle == 'collor':\n",
    "        name = 'Fernando Collor'\n",
    "    if row.twitter_handle == 'geraldoalckmin':\n",
    "        name = 'Geraldo Alckmin'\n",
    "    if row.twitter_handle == 'GuilhermeBoulos':\n",
    "        name = 'Guilherme Boulos'\n",
    "    if row.twitter_handle == 'jairbolsonaro':\n",
    "        name = 'Jair Bolsonaro'\n",
    "    if row.twitter_handle == 'joaoamoedonovo':\n",
    "        name = 'João Amoedo'\n",
    "    if row.twitter_handle == 'lulapelobrasil':\n",
    "        name = 'Lula'\n",
    "    if (row.twitter_handle == 'ManuelaDavila') or (row.twitter_handle == 'manudeputada'):\n",
    "        name = \"Manuela d'Ávila\"\n",
    "    if row.twitter_handle == 'RodrigoMaia':\n",
    "        name = 'Rodrigo Maia'\n",
    "    if row.twitter_handle == 'silva_marina':\n",
    "        name = 'Marina Silva'\n",
    "    if row.twitter_handle == 'MichelTemer':\n",
    "        name = 'Michel Temer'\n",
    "    if row.twitter_handle == 'alvarodias_':\n",
    "        name = 'Álvaro Dias'\n",
    "    if row.twitter_handle == 'jaqueswagner':\n",
    "        name = 'Jaques Wagner'\n",
    "    if row.twitter_handle == 'Haddad_Fernando':\n",
    "        name = 'Fernando Haddad'\n",
    "    if row.twitter_handle == 'meirelles':\n",
    "        name = 'Henrique Meirelles'\n",
    "    if row.twitter_handle == 'joaquimboficial':\n",
    "        name = 'Joaquim Barbosa'\n",
    "        \n",
    "    return pd.Series({'name':name})\n",
    "\n",
    "df['name'] = df.apply(get_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salva o df com todos os tweets a partir da data de análise para um arquivo\n",
    "# Esse arquivo será usado para mostrar os tweets na visualização\n",
    "df.to_csv('../data/bases-recorte-temporal/todos-candidatos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cria vários dataframes diferentes, um para os tweets de cada candidato\n",
    "lula = df[df.name=='Lula']\n",
    "ciro = df[df.name=='Ciro Gomes']\n",
    "bolsonaro = df[df.name=='Jair Bolsonaro']\n",
    "marina = df[df.name=='Marina Silva']\n",
    "alckmin = df[df.name=='Geraldo Alckmin']\n",
    "temer = df[df.name=='Michel Temer']\n",
    "maia = df[df.name=='Rodrigo Maia']\n",
    "collor = df[df.name=='Fernando Collor']\n",
    "amoedo = df[df.name=='João Amoedo']\n",
    "boulos = df[df.name=='Guilherme Boulos']\n",
    "manuela = df[df.name==\"Manuela d'Ávila\"]\n",
    "alvaro = df[df.name==\"Álvaro Dias\"]\n",
    "wagner = df[df.name==\"Jaques Wagner\"]\n",
    "haddad = df[df.name==\"Fernando Haddad\"]\n",
    "meirelles = df[df.name==\"Henrique Meirelles\"]\n",
    "joaquim = df[df.name==\"Joaquim Barbosa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salva o df com todos os tweets a partir da data de análise para um arquivo .csv\n",
    "# Esse arquivo será usado para mostrar os tweets na visualização\n",
    "lula.to_csv('../data/bases-recorte-temporal/lula.csv', index=False)\n",
    "ciro.to_csv('../data/bases-recorte-temporal/ciro.csv', index=False)\n",
    "bolsonaro.to_csv('../data/bases-recorte-temporal/bolsonaro.csv', index=False)\n",
    "marina.to_csv('../data/bases-recorte-temporal/marina.csv', index=False)\n",
    "alckmin.to_csv('../data/bases-recorte-temporal/alckmin.csv', index=False)\n",
    "temer.to_csv('../data/bases-recorte-temporal/temer.csv', index=False)\n",
    "maia.to_csv('../data/bases-recorte-temporal/maia.csv', index=False)\n",
    "collor.to_csv('../data/bases-recorte-temporal/collor.csv', index=False)\n",
    "amoedo.to_csv('../data/bases-recorte-temporal/amoedo.csv', index=False)\n",
    "boulos.to_csv('../data/bases-recorte-temporal/boulos.csv', index=False)\n",
    "manuela.to_csv('../data/bases-recorte-temporal/manuela.csv', index=False)\n",
    "alvaro.to_csv('../data/bases-recorte-temporal/alvaro.csv', index=False)\n",
    "wagner.to_csv('../data/bases-recorte-temporal/wagner.csv', index=False)\n",
    "haddad.to_csv('../data/bases-recorte-temporal/haddad.csv', index=False)\n",
    "meirelles.to_csv('../data/bases-recorte-temporal/meirelles.csv', index=False)\n",
    "joaquim.to_csv('../data/bases-recorte-temporal/joaquim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emojistring = '''😀 😁 😂 🤣 😃 😄 😅 😆 😉 😊 😋 😎 😍 😘 😗 😙 😚 ☺️ 🙂 🤗 🤩 🤔 🤨 😐 😑 😶 🙄 😏 😣 😥 😮 🤐 😯 😪 😫 😴 😌 😛 😜 😝 🤤 😒 😓 😔 😕 🙃 🤑 😲 ☹️ 🙁 😖 😞 😟 😤 😢 😭 😦 😧 😨 😩 🤯 😬 😰 😱 😳 🤪 😵 😡 😠 🤬 😷 🤒 🤕 🤢 🤮 🤧 😇 🤠 🤡 🤥 🤫 🤭 🧐 🤓 😈 👿 👹 👺 💀 👻 👽 🤖 💩 😺 😸 😹 😻 😼 😽 🙀 😿 😾\n",
    "👶 👦 👧 👨 👩 👴 👵 👨‍⚕️ 👩‍⚕️ 👨‍🎓 👩‍🎓 👨‍⚖️ 👩‍⚖️ 👨‍🌾 👩‍🌾 👨‍🍳 👩‍🍳 👨‍🔧 👩‍🔧 👨‍🏭 👩‍🏭 👨‍💼 👩‍💼 👨‍🔬 👩‍🔬 👨‍💻 👩‍💻 👨‍🎤 👩‍🎤 👨‍🎨 👩‍🎨 👨‍✈️ 👩‍✈️ 👨‍🚀 👩‍🚀 👨‍🚒 👩‍🚒 👮 👮‍♂️ 👮‍♀️ 🕵 🕵️‍♂️ 🕵️‍♀️ 💂 💂‍♂️ 💂‍♀️ 👷 👷‍♂️ 👷‍♀️ 🤴 👸 👳 👳‍♂️ 👳‍♀️ 👲 🧕 🧔 👱 👱‍♂️ 👱‍♀️ 🤵 👰 🤰 🤱 👼 🎅 🤶 🧙‍♀️ 🧙‍♂️ 🧚‍♀️ 🧚‍♂️ 🧛‍♀️ 🧛‍♂️ 🧜‍♀️ 🧜‍♂️ 🧝‍♀️ 🧝‍♂️ 🧞‍♀️ 🧞‍♂️ 🧟‍♀️ 🧟‍♂️ 🙍 🙍‍♂️ 🙍‍♀️ 🙎 🙎‍♂️ 🙎‍♀️ 🙅 🙅‍♂️ 🙅‍♀️ 🙆 🙆‍♂️ 🙆‍♀️ 💁 💁‍♂️ 💁‍♀️ 🙋 🙋‍♂️ 🙋‍♀️ 🙇 🙇‍♂️ 🙇‍♀️ 🤦 🤦‍♂️ 🤦‍♀️ 🤷 🤷‍♂️ 🤷‍♀️ 💆 💆‍♂️ 💆‍♀️ 💇 💇‍♂️ 💇‍♀️ 🚶 🚶‍♂️ 🚶‍♀️ 🏃 🏃‍♂️ 🏃‍♀️ 💃 🕺 👯 👯‍♂️ 👯‍♀️ 🧖‍♀️ 🧖‍♂️ 🕴 🗣 👤 👥 👫 👬 👭 💏 👨‍❤️‍💋‍👨 👩‍❤️‍💋‍👩 💑 👨‍❤️‍👨 👩‍❤️‍👩 👪 👨‍👩‍👦 👨‍👩‍👧 👨‍👩‍👧‍👦 👨‍👩‍👦‍👦 👨‍👩‍👧‍👧 👨‍👨‍👦 👨‍👨‍👧 👨‍👨‍👧‍👦 👨‍👨‍👦‍👦 👨‍👨‍👧‍👧 👩‍👩‍👦 👩‍👩‍👧 👩‍👩‍👧‍👦 👩‍👩‍👦‍👦 👩‍👩‍👧‍👧 👨‍👦 👨‍👦‍👦 👨‍👧 👨‍👧‍👦 👨‍👧‍👧 👩‍👦 👩‍👦‍👦 👩‍👧 👩‍👧‍👦 👩‍👧‍👧 🤳 💪 👈 👉 ☝️ 👆 🖕 👇 ✌️ 🤞 🖖 🤘 🖐 ✋ 👌 👍 👎 ✊ 👊 🤛 🤜 🤚 👋 🤟 ✍️ 👏 👐 🙌 🤲 🙏 🤝 💅 👂 👃 👣 👀 👁 🧠 👅 👄 💋\n",
    "\n",
    "👓 🕶 👔 👕 👖 🧣 🧤 🧥 🧦 👗 👘 👙 👚 👛 👜 👝 🎒 👞 👟 👠 👡 👢 👑 👒 🎩 🎓 🧢 ⛑ 💄 💍 🌂 💼\n",
    "👐🏻 🙌🏻 👏🏻 🙏🏻 👍🏻 👎🏻 👊🏻 ✊🏻 🤛🏻 🤜🏻 🤞🏻 ✌🏻 🤘🏻 👌🏻 👈🏻 👉🏻 👆🏻 👇🏻 ☝🏻 ✋🏻 🤚🏻 🖐🏻 🖖🏻 👋🏻 🤙🏻 💪🏻 🖕🏻 ✍🏻 🤳🏻 💅🏻 👂🏻 👃🏻 👶🏻 👦🏻 👧🏻 👨🏻 👩🏻 👱🏻‍♀️ 👱🏻 👴🏻 👵🏻 👲🏻 👳🏻‍♀️ 👳🏻 👮🏻‍♀️ 👮🏻 👷🏻‍♀️ 👷🏻 💂🏻‍♀️ 💂🏻 🕵🏻‍♀️ 🕵🏻 👩🏻‍⚕️ 👨🏻‍⚕️ 👩🏻‍🌾 👨🏻‍🌾 👩🏻‍🍳 👨🏻‍🍳 👩🏻‍🎓 👨🏻‍🎓 👩🏻‍🎤 👨🏻‍🎤 👩🏻‍🏫 👨🏻‍🏫 👩🏻‍🏭 👨🏻‍🏭 👩🏻‍💻 👨🏻‍💻 👩🏻‍💼 👨🏻‍💼 👩🏻‍🔧 👨🏻‍🔧 👩🏻‍🔬 👨🏻‍🔬 👩🏻‍🎨 👨🏻‍🎨 👩🏻‍🚒 👨🏻‍🚒 👩🏻‍✈️ 👨🏻‍✈️ 👩🏻‍🚀 👨🏻‍🚀 👩🏻‍⚖️ 👨🏻‍⚖️ 🤶🏻 🎅🏻 👸🏻 🤴🏻 👰🏻 🤵🏻 👼🏻 🤰🏻 🙇🏻‍♀️ 🙇🏻 💁🏻 💁🏻‍♂️ 🙅🏻 🙅🏻‍♂️ 🙆🏻 🙆🏻‍♂️ 🙋🏻 🙋🏻‍♂️ 🤦🏻‍♀️ 🤦🏻‍♂️ 🤷🏻‍♀️ 🤷🏻‍♂️ 🙎🏻 🙎🏻‍♂️ 🙍🏻 🙍🏻‍♂️ 💇🏻 💇🏻‍♂️ 💆🏻 💆🏻‍♂️ 🕴🏻 💃🏻 🕺🏻 🚶🏻‍♀️ 🚶🏻 🏃🏻‍♀️ 🏃🏻 🏋🏻‍♀️ 🏋🏻 🤸🏻‍♀️ 🤸🏻‍♂️ ⛹🏻‍♀️ ⛹🏻 🤾🏻‍♀️ 🤾🏻‍♂️ 🏌🏻‍♀️ 🏌🏻 🏄🏻‍♀️ 🏄🏻 🏊🏻‍♀️ 🏊🏻 🤽🏻‍♀️ 🤽🏻‍♂️ 🚣🏻‍♀️ 🚣🏻 🏇🏻 🚴🏻‍♀️ 🚴🏻 🚵🏻‍♀️ 🚵🏻 🤹🏻‍♀️ 🤹🏻‍♂️ 🛀🏻\n",
    "\n",
    "👐🏼 🙌🏼 👏🏼 🙏🏼 👍🏼 👎🏼 👊🏼 ✊🏼 🤛🏼 🤜🏼 🤞🏼 ✌🏼 🤘🏼 👌🏼 👈🏼 👉🏼 👆🏼 👇🏼 ☝🏼 ✋🏼 🤚🏼 🖐🏼 🖖🏼 👋🏼 🤙🏼 💪🏼 🖕🏼 ✍🏼 🤳🏼 💅🏼 👂🏼 👃🏼 👶🏼 👦🏼 👧🏼 👨🏼 👩🏼 👱🏼‍♀️ 👱🏼 👴🏼 👵🏼 👲🏼 👳🏼‍♀️ 👳🏼 👮🏼‍♀️ 👮🏼 👷🏼‍♀️ 👷🏼 💂🏼‍♀️ 💂🏼 🕵🏼‍♀️ 🕵🏼 👩🏼‍⚕️ 👨🏼‍⚕️ 👩🏼‍🌾 👨🏼‍🌾 👩🏼‍🍳 👨🏼‍🍳 👩🏼‍🎓 👨🏼‍🎓 👩🏼‍🎤 👨🏼‍🎤 👩🏼‍🏫 👨🏼‍🏫 👩🏼‍🏭 👨🏼‍🏭 👩🏼‍💻 👨🏼‍💻 👩🏼‍💼 👨🏼‍💼 👩🏼‍🔧 👨🏼‍🔧 👩🏼‍🔬 👨🏼‍🔬 👩🏼‍🎨 👨🏼‍🎨 👩🏼‍🚒 👨🏼‍🚒 👩🏼‍✈️ 👨🏼‍✈️ 👩🏼‍🚀 👨🏼‍🚀 👩🏼‍⚖️ 👨🏼‍⚖️ 🤶🏼 🎅🏼 👸🏼 🤴🏼 👰🏼 🤵🏼 👼🏼 🤰🏼 🙇🏼‍♀️ 🙇🏼 💁🏼 💁🏼‍♂️ 🙅🏼 🙅🏼‍♂️ 🙆🏼 🙆🏼‍♂️ 🙋🏼 🙋🏼‍♂️ 🤦🏼‍♀️ 🤦🏼‍♂️ 🤷🏼‍♀️ 🤷🏼‍♂️ 🙎🏼 🙎🏼‍♂️ 🙍🏼 🙍🏼‍♂️ 💇🏼 💇🏼‍♂️ 💆🏼 💆🏼‍♂️ 🕴🏼 💃🏼 🕺🏼 🚶🏼‍♀️ 🚶🏼 🏃🏼‍♀️ 🏃🏼 🏋🏼‍♀️ 🏋🏼 🤸🏼‍♀️ 🤸🏼‍♂️ ⛹🏼‍♀️ ⛹🏼 🤾🏼‍♀️ 🤾🏼‍♂️ 🏌🏼‍♀️ 🏌🏼 🏄🏼‍♀️ 🏄🏼 🏊🏼‍♀️ 🏊🏼 🤽🏼‍♀️ 🤽🏼‍♂️ 🚣🏼‍♀️ 🚣🏼 🏇🏼 🚴🏼‍♀️ 🚴🏼 🚵🏼‍♀️ 🚵🏻 🤹🏼‍♀️ 🤹🏼‍♂️ 🛀🏼\n",
    "\n",
    "👐🏽 🙌🏽 👏🏽 🙏🏽 👍🏽 👎🏽 👊🏽 ✊🏽 🤛🏽 🤜🏽 🤞🏽 ✌🏽 🤘🏽 👌🏽 👈🏽 👉🏽 👆🏽 👇🏽 ☝🏽 ✋🏽 🤚🏽 🖐🏽 🖖🏽 👋🏽 🤙🏽 💪🏽 🖕🏽 ✍🏽 🤳🏽 💅🏽 👂🏽 👃🏽 👶🏽 👦🏽 👧🏽 👨🏽 👩🏽 👱🏽‍♀️ 👱🏽 👴🏽 👵🏽 👲🏽 👳🏽‍♀️ 👳🏽 👮🏽‍♀️ 👮🏽 👷🏽‍♀️ 👷🏽 💂🏽‍♀️ 💂🏽 🕵🏽‍♀️ 🕵🏽 👩🏽‍⚕️ 👨🏽‍⚕️ 👩🏽‍🌾 👨🏽‍🌾 👩🏽‍🍳 👨🏽‍🍳 👩🏽‍🎓 👨🏽‍🎓 👩🏽‍🎤 👨🏽‍🎤 👩🏽‍🏫 👨🏽‍🏫 👩🏽‍🏭 👨🏽‍🏭 👩🏽‍💻 👨🏽‍💻 👩🏽‍💼 👨🏽‍💼 👩🏽‍🔧 👨🏽‍🔧 👩🏽‍🔬 👨🏽‍🔬 👩🏽‍🎨 👨🏽‍🎨 👩🏽‍🚒 👨🏽‍🚒 👩🏽‍✈️ 👨🏽‍✈️ 👩🏽‍🚀 👨🏽‍🚀 👩🏽‍⚖️ 👨🏽‍⚖️ 🤶🏽 🎅🏽 👸🏽 🤴🏽 👰🏽 🤵🏽 👼🏽 🤰🏽 🙇🏽‍♀️ 🙇🏽 💁🏽 💁🏽‍♂️ 🙅🏽 🙅🏽‍♂️ 🙆🏽 🙆🏽‍♂️ 🙋🏽 🙋🏽‍♂️ 🤦🏽‍♀️ 🤦🏽‍♂️ 🤷🏽‍♀️ 🤷🏽‍♂️ 🙎🏽 🙎🏽‍♂️ 🙍🏽 🙍🏽‍♂️ 💇🏽 💇🏽‍♂️ 💆🏽 💆🏽‍♂️ 🕴🏼 💃🏽 🕺🏽 🚶🏽‍♀️ 🚶🏽 🏃🏽‍♀️ 🏃🏽 🏋🏽‍♀️ 🏋🏽 🤸🏽‍♀️ 🤸🏽‍♂️ ⛹🏽‍♀️ ⛹🏽 🤾🏽‍♀️ 🤾🏽‍♂️ 🏌🏽‍♀️ 🏌🏽 🏄🏽‍♀️ 🏄🏽 🏊🏽‍♀️ 🏊🏽 🤽🏽‍♀️ 🤽🏽‍♂️ 🚣🏽‍♀️ 🚣🏽 🏇🏽 🚴🏽‍♀️ 🚴🏽 🚵🏽‍♀️ 🚵🏽 🤹🏽‍♀️ 🤹🏽‍♂️ 🛀🏽\n",
    "\n",
    "👐🏾 🙌🏾 👏🏾 🙏🏾 👍🏾 👎🏾 👊🏾 ✊🏾 🤛🏾 🤜🏾 🤞🏾 ✌🏾 🤘🏾 👌🏾 👈🏾 👉🏾 👆🏾 👇🏾 ☝🏾 ✋🏾 🤚🏾 🖐🏾 🖖🏾 👋🏾 🤙🏾 💪🏾 🖕🏾 ✍🏾 🤳🏾 💅🏾 👂🏾 👃🏾 👶🏾 👦🏾 👧🏾 👨🏾 👩🏾 👱🏾‍♀️ 👱🏾 👴🏾 👵🏾 👲🏾 👳🏾‍♀️ 👳🏾 👮🏾‍♀️ 👮🏾 👷🏾‍♀️ 👷🏾 💂🏾‍♀️ 💂🏾 🕵🏾‍♀️ 🕵🏾 👩🏾‍⚕️ 👨🏾‍⚕️ 👩🏾‍🌾 👨🏾‍🌾 👩🏾‍🍳 👨🏾‍🍳 👩🏾‍🎓 👨🏾‍🎓 👩🏾‍🎤 👨🏾‍🎤 👩🏾‍🏫 👨🏾‍🏫 👩🏾‍🏭 👨🏾‍🏭 👩🏾‍💻 👨🏾‍💻 👩🏾‍💼 👨🏾‍💼 👩🏾‍🔧 👨🏾‍🔧 👩🏾‍🔬 👨🏾‍🔬 👩🏾‍🎨 👨🏾‍🎨 👩🏾‍🚒 👨🏾‍🚒 👩🏾‍✈️ 👨🏾‍✈️ 👩🏾‍🚀 👨🏾‍🚀 👩🏾‍⚖️ 👨🏾‍⚖️ 🤶🏾 🎅🏾 👸🏾 🤴🏾 👰🏾 🤵🏾 👼🏾 🤰🏾 🙇🏾‍♀️ 🙇🏾 💁🏾 💁🏾‍♂️ 🙅🏾 🙅🏾‍♂️ 🙆🏾 🙆🏾‍♂️ 🙋🏾 🙋🏾‍♂️ 🤦🏾‍♀️ 🤦🏾‍♂️ 🤷🏾‍♀️ 🤷🏾‍♂️ 🙎🏾 🙎🏾‍♂️ 🙍🏾 🙍🏾‍♂️ 💇🏾 💇🏾‍♂️ 💆🏾 💆🏾‍♂️ 🕴🏾 💃🏾 🕺🏾 🚶🏾‍♀️ 🚶🏾 🏃🏾‍♀️ 🏃🏾 🏋🏾‍♀️ 🏋🏾 🤸🏾‍♀️ 🤸🏾‍♂️ ⛹🏾‍♀️ ⛹🏾 🤾🏾‍♀️ 🤾🏾‍♂️ 🏌🏾‍♀️ 🏌🏾 🏄🏾‍♀️ 🏄🏾 🏊🏾‍♀️ 🏊🏾 🤽🏾‍♀️ 🤽🏾‍♂️ 🚣🏾‍♀️ 🚣🏾 🏇🏾 🚴🏾‍♀️ 🚴🏾 🚵🏾‍♀️ 🚵🏾 🤹🏾‍♀️ 🤹🏾‍♂️ 🛀🏾\n",
    "\n",
    "👐🏿 🙌🏿 👏🏿 🙏🏿 👍🏿 👎🏿 👊🏿 ✊🏿 🤛🏿 🤜🏿 🤞🏿 ✌🏿 🤘🏿 👌🏿 👈🏿 👉🏿 👆🏿 👇🏿 ☝🏿 ✋🏿 🤚🏿 🖐🏿 🖖🏿 👋🏿 🤙🏿 💪🏿 🖕🏿 ✍🏿 🤳🏿 💅🏿 👂🏿 👃🏿 👶🏿 👦🏿 👧🏿 👨🏿 👩🏿 👱🏿‍♀️ 👱🏿 👴🏿 👵🏿 👲🏿 👳🏿‍♀️ 👳🏿 👮🏿‍♀️ 👮🏿 👷🏿‍♀️ 👷🏿 💂🏿‍♀️ 💂🏿 🕵🏿‍♀️ 🕵🏿 👩🏿‍⚕️ 👨🏿‍⚕️ 👩🏿‍🌾 👨🏿‍🌾 👩🏿‍🍳 👨🏿‍🍳 👩🏿‍🎓 👨🏿‍🎓 👩🏿‍🎤 👨🏿‍🎤 👩🏿‍🏫 👨🏿‍🏫 👩🏿‍🏭 👨🏿‍🏭 👩🏿‍💻 👨🏿‍💻 👩🏿‍💼 👨🏿‍💼 👩🏿‍🔧 👨🏿‍🔧 👩🏿‍🔬 👨🏿‍🔬 👩🏿‍🎨 👨🏿‍🎨 👩🏿‍🚒 👨🏿‍🚒 👩🏿‍✈️ 👨🏿‍✈️ 👩🏿‍🚀 👨🏿‍🚀 👩🏿‍⚖️ 👨🏿‍⚖️ 🤶🏿 🎅🏿 👸🏿 🤴🏿 👰🏿 🤵🏿 👼🏿 🤰🏿 🙇🏿‍♀️ 🙇🏿 💁🏿 💁🏿‍♂️ 🙅🏿 🙅🏿‍♂️ 🙆🏿 🙆🏿‍♂️ 🙋🏿 🙋🏿‍♂️ 🤦🏿‍♀️ 🤦🏿‍♂️ 🤷🏿‍♀️ 🤷🏿‍♂️ 🙎🏿 🙎🏿‍♂️ 🙍🏿 🙍🏿‍♂️ 💇🏿 💇🏿‍♂️ 💆🏿 💆🏿‍♂️ 🕴🏿 💃🏿 🕺🏿 🚶🏿‍♀️ 🚶🏿 🏃🏿‍♀️ 🏃🏿 🏋🏿‍♀️ 🏋🏿 🤸🏿‍♀️ 🤸🏿‍♂️ ⛹🏿‍♀️ ⛹🏿 🤾🏿‍♀️ 🤾🏿‍♂️ 🏌🏿‍♀️ 🏌🏿 🏄🏿‍♀️ 🏄🏿 🏊🏿‍♀️ 🏊🏿 🤽🏿‍♀️ 🤽🏿‍♂️ 🚣🏿‍♀️ 🚣🏿 🏇🏿 🚴🏿‍♀️ 🚴🏿 🚵🏿‍♀️ 🚵🏿 🤹🏿‍♀️ 🤹🏿‍♂️ 🛀🏿\n",
    "\n",
    "🐶 🐱 🐭 🐹 🐰 🦊 🐻 🐼 🐨 🐯 🦁 🐮 🐷 🐽 🐸 🐵 🙊 🙉 🙊 🐒 🐔 🐧 🐦 🐤 🐣 🐥 🦆 🦅 🦉 🦇 🐺 🐗 🐴 🦄 🐝 🐛 🦋 🐌 🐚 🐞 🐜 🕷 🕸 🐢 🐍 🦎 🦂 🦀 🦑 🐙 🦐 🐠 🐟 🐡 🐬 🦈 🐳 🐋 🐊 🐆 🐅 🐃 🐂 🐄 🦌 🐪 🐫 🐘 🦏 🦍 🐎 🐖 🐐 🐏 🐑 🐕 🐩 🐈 🐓 🦃 🕊 🐇 🐁 🐀 🐿 🐾 🐉 🐲 🌵 🎄 🌲 🌳 🌴 🌱 🌿 ☘️ 🍀 🎍 🎋 🍃 🍂 🍁 🍄 🌾 💐 🌷 🌹 🥀 🌻 🌼 🌸 🌺 🌎 🌍 🌏 🌕 🌖 🌗 🌘 🌑 🌒 🌓 🌔 🌚 🌝 🌞 🌛 🌜 🌙 💫 ⭐️ 🌟 ✨ ⚡️ 🔥 💥 ☄️ ☀️ 🌤 ⛅️ 🌥 🌦 🌈 ☁️ 🌧 ⛈ 🌩 🌨 ☃️ ⛄️ ❄️ 🌬 💨 🌪 🌫 🌊 💧 💦 ☔️\n",
    "\n",
    "🍏 🍎 🍐 🍊 🍋 🍌 🍉 🍇 🍓 🍈 🍒 🍑 🍍 🥝 🥑 🍅 🍆 🥒 🥕 🌽 🌶 🥔 🍠 🌰 🥜 🍯 🥐 🍞 🥖 🧀 🥚 🍳 🥓 🥞 🍤 🍗 🍖 🍕 🌭 🍔 🍟 🥙 🌮 🌯 🥗 🥘 🍝 🍜 🍲 🍥 🍣 🍱 🍛 🍚 🍙 🍘 🍢 🍡 🍧 🍨 🍦 🍰 🎂 🍮 🍭 🍬 🍫 🍿 🍩 🍪 🥛 🍼 ☕️ 🍵 🍶 🍺 🍻 🥂 🍷 🥃 🍸 🍹 🍾 🥄 🍴 🍽\n",
    "\n",
    "⚽️ 🏀 🏈 ⚾️ 🎾 🏐 🏉 🎱 🏓 🏸 🥅 🏒 🏑 🏏 ⛳️ 🏹 🎣 🥊 🥋 ⛸ 🎿 ⛷ 🏂 🏋️‍♀️ 🏋️ 🤺 🤼‍♀️ 🤼‍♂️ 🤸‍♀️ 🤸‍♂️ ⛹️‍♀️ ⛹️ 🤾‍♀️ 🤾‍♂️ 🏌️‍♀️ 🏌️ 🏄‍♀️ 🏄 🏊‍♀️ 🏊 🤽‍♀️ 🤽‍♂️ 🚣‍♀️ 🚣 🏇 🚴‍♀️ 🚴 🚵‍♀️ 🚵 🎽 🏅 🎖 🥇 🥈 🥉 🏆 🏵 🎗 🎫 🎟 🎪 🤹‍♀️ 🤹‍♂️ 🎭 🎨 🎬 🎤 🎧 🎼 🎹 🥁 🎷 🎺 🎸 🎻 🎲 🎯 🎳 🎮 🎰\n",
    "\n",
    "🚗 🚕 🚙 🚌 🚎 🏎 🚓 🚑 🚒 🚐 🚚 🚛 🚜 🛴 🚲 🛵 🏍 🚨 🚔 🚍 🚘 🚖 🚡 🚠 🚟 🚃 🚋 🚞 🚝 🚄 🚅 🚈 🚂 🚆 🚇 🚊 🚉 🚁 🛩 ✈️ 🛫 🛬 🚀 🛰 💺 🛶 ⛵️ 🛥 🚤 🛳 ⛴ 🚢 ⚓️ 🚧 ⛽️ 🚏 🚦 🚥 🗺 🗿 🗽 ⛲️ 🗼 🏰 🏯 🏟 🎡 🎢 🎠 ⛱ 🏖 🏝 ⛰ 🏔 🗻 🌋 🏜 🏕 ⛺️ 🛤 🛣 🏗 🏭 🏠 🏡 🏘 🏚 🏢 🏬 🏣 🏤 🏥 🏦 🏨 🏪 🏫 🏩 💒 🏛 ⛪️ 🕌 🕍 🕋 ⛩ 🗾 🎑 🏞 🌅 🌄 🌠 🎇 🎆 🌇 🌆 🏙 🌃 🌌 🌉 🌁\n",
    "\n",
    "⌚️ 📱 📲 💻 ⌨️ 🖥 🖨 🖱 🖲 🕹 🗜 💽 💾 💿 📀 📼 📷 📸 📹 🎥 📽 🎞 📞 ☎️ 📟 📠 📺 📻 🎙 🎚 🎛 ⏱ ⏲ ⏰ 🕰 ⌛️ ⏳ 📡 🔋 🔌 💡 🔦 🕯 🗑 🛢 💸 💵 💴 💶 💷 💰 💳 💎 ⚖️ 🔧 🔨 ⚒ 🛠 ⛏ 🔩 ⚙️ ⛓ 🔫 💣 🔪 🗡 ⚔️ 🛡 🚬 ⚰️ ⚱️ 🏺 🔮 📿 💈 ⚗️ 🔭 🔬 🕳 💊 💉 🌡 🚽 🚰 🚿 🛁 🛀 🛎 🔑 🗝 🚪 🛋 🛏 🛌 🖼 🛍 🛒 🎁 🎈 🎏 🎀 🎊 🎉 🎎 🏮 🎐 ✉️ 📩 📨 📧 💌 📥 📤 📦 🏷 📪 📫 📬 📭 📮 📯 📜 📃 📄 📑 📊 📈 📉 🗒 🗓 📆 📅 📇 🗃 🗳 🗄 📋 📁 📂 🗂 🗞 📰 📓 📔 📒 📕 📗 📘 📙 📚 📖 🔖 🔗 📎 🖇 📐 📏 📌 📍 📌 🎌 🏳️ 🏴 🏁 🏳️‍🌈 ✂️ 🖊 🖋 ✒️ 🖌 🖍 📝 ✏️ 🔍 🔎 🔏 🔐 🔒 🔓\n",
    "\n",
    "❤️ 💛 💚 💙 💜 🖤 💔 ❣️ 💕 💞 💓 💗 💖 💘 💝 💟 ☮️ ✝️ ☪️ 🕉 ☸️ ✡️ 🔯 🕎 ☯️ ☦️ 🛐 ⛎ ♈️ ♉️ ♊️ ♋️ ♌️ ♍️ ♎️ ♏️ ♐️ ♑️ ♒️ ♓️ 🆔 ⚛️ 🉑 ☢️ ☣️ 📴 📳 🈶 🈚️ 🈸 🈺 🈷️ ✴️ 🆚 💮 🉐 ㊙️ ㊗️ 🈴 🈵 🈹 🈲 🅰️ 🅱️ 🆎 🆑 🅾️ 🆘 ❌ ⭕️ 🛑 ⛔️ 📛 🚫 💯 💢 ♨️ 🚷 🚯 🚳 🚱 🔞 📵 🚭 ❗️ ❕ ❓ ❔ ‼️ ⁉️ 🔅 🔆 〽️ ⚠️ 🚸 🔱 ⚜️ 🔰 ♻️ ✅ 🈯️ 💹 ❇️ ✳️ ❎ 🌐 💠 Ⓜ️ 🌀 💤 🏧 🚾 ♿️ 🅿️ 🈳 🈂️ 🛂 🛃 🛄 🛅 🚹 🚺 🚼 🚻 🚮 🎦 📶 🈁 🔣 ℹ️ 🔤 🔡 🔠 🆖 🆗 🆙 🆒 🆕 🆓 0️⃣ 1️⃣ 2️⃣ 3️⃣ 4️⃣ 5️⃣ 6️⃣ 7️⃣ 8️⃣ 9️⃣ 🔟 🔢 #️⃣ *️⃣ ▶️ ⏸ ⏯ ⏹ ⏺ ⏭ ⏮ ⏩ ⏪ ⏫ ⏬ ◀️ 🔼 🔽 ➡️ ⬅️ ⬆️ ⬇️ ↗️ ↘️ ↙️ ↖️ ↕️ ↔️ ↪️ ↩️ ⤴️ ⤵️ 🔀 🔁 🔂 🔄 🔃 🎵 🎶 ➕ ➖ ➗ ✖️ 💲 💱 ™️ ©️ ®️ 〰️ ➰ ➿ 🔚 🔙 🔛 🔝 ✔️ ☑️ 🔘 ⚪️ ⚫️ 🔴 🔵 🔺 🔻 🔸 🔹 🔶 🔷 🔳 🔲 ▪️ ▫️ ◾️ ◽️ ◼️ ◻️ ⬛️ ⬜️ 🔈 🔇 🔉 🔊 🔔 🔕 📣 📢 👁‍🗨 💬 💭 🗯 ♠️ ♣️ ♥️ ♦️ 🃏 🎴 🀄️ 🕐 🕑 🕒 🕓 🕔 🕕 🕖 🕗 🕘 🕙 🕚 🕛 🕜 🕝 🕞 🕟 🕠 🕡 🕢 🕣 🕤 🕥 🕦 🕧\n",
    "\n",
    "🏳️ 🏴 🏁 🚩 🏳️‍🌈 🇦🇫 🇦🇽 🇦🇱 🇩🇿 🇦🇸 🇦🇩 🇦🇴 🇦🇮 🇦🇶 🇦🇬 🇦🇷 🇦🇲 🇦🇼 🇦🇺 🇦🇹 🇦🇿 🇧🇸 🇧🇭 🇧🇩 🇧🇧 🇧🇾 🇧🇪 🇧🇿 🇧🇯 🇧🇲 🇧🇹 🇧🇴 🇧🇦 🇧🇼 🇧🇷 🇮🇴 🇻🇬 🇧🇳 🇧🇬 🇧🇫 🇧🇮 🇰🇭 🇨🇲 🇨🇦 🇮🇨 🇨🇻 🇧🇶 🇰🇾 🇨🇫 🇹🇩 🇨🇱 🇨🇳 🇨🇽 🇨🇨 🇨🇴 🇰🇲 🇨🇬 🇨🇩 🇨🇰 🇨🇷 🇨🇮 🇭🇷 🇨🇺 🇨🇼 🇨🇾 🇨🇿 🇩🇰 🇩🇯 🇩🇲 🇩🇴 🇪🇨 🇪🇬 🇸🇻 🇬🇶 🇪🇷 🇪🇪 🇪🇹 🇪🇺 🇫🇰 🇫🇴 🇫🇯 🇫🇮 🇫🇷 🇬🇫 🇵🇫 🇹🇫 🇬🇦 🇬🇲 🇬🇪 🇩🇪 🇬🇭 🇬🇮 🇬🇷 🇬🇱 🇬🇩 🇬🇵 🇬🇺 🇬🇹 🇬🇬 🇬🇳 🇬🇼 🇬🇾 🇭🇹 🇭🇳 🇭🇰 🇭🇺 🇮🇸 🇮🇳 🇮🇩 🇮🇷 🇮🇶 🇮🇪 🇮🇲 🇮🇱 🇮🇹 🇯🇲 🇯🇵 🎌 🇯🇪 🇯🇴 🇰🇿 🇰🇪 🇰🇮 🇽🇰 🇰🇼 🇰🇬 🇱🇦 🇱🇻 🇱🇧 🇱🇸 🇱🇷 🇱🇾 🇱🇮 🇱🇹 🇱🇺 🇲🇴 🇲🇰 🇲🇬 🇲🇼 🇲🇾 🇲🇻 🇲🇱 🇲🇹 🇲🇭 🇲🇶 🇲🇷 🇲🇺 🇾🇹 🇲🇽 🇫🇲 🇲🇩 🇲🇨 🇲🇳 🇲🇪 🇲🇸 🇲🇦 🇲🇿 🇲🇲 🇳🇦 🇳🇷 🇳🇵 🇳🇱 🇳🇨 🇳🇿 🇳🇮 🇳🇪 🇳🇬 🇳🇺 🇳🇫 🇰🇵 🇲🇵 🇳🇴 🇴🇲 🇵🇰 🇵🇼 🇵🇸 🇵🇦 🇵🇬 🇵🇾 🇵🇪 🇵🇭 🇵🇳 🇵🇱 🇵🇹 🇵🇷 🇶🇦 🇷🇪 🇷🇴 🇷🇺 🇷🇼 🇼🇸 🇸🇲 🇸🇦 🇸🇳 🇷🇸 🇸🇨 🇸🇱 🇸🇬 🇸🇽 🇸🇰 🇸🇮 🇬🇸 🇸🇧 🇸🇴 🇿🇦 🇰🇷 🇸🇸 🇪🇸 🇱🇰 🇧🇱 🇸🇭 🇰🇳 🇱🇨 🇵🇲 🇻🇨 🇸🇩 🇸🇷 🇸🇿 🇸🇪 🇨🇭 🇸🇾 🇹🇼 🇹🇯 🇹🇿 🇹🇭 🇹🇱 🇹🇬 🇹🇰 🇹🇴 🇹🇹 🇹🇳 🇹🇷 🇹🇲 🇹🇨 🇹🇻 🇻🇮 🇺🇬 🇺🇦 🇦🇪 🇬🇧 🏴󠁧󠁢󠁥󠁮󠁧󠁿 🏴󠁧󠁢󠁳󠁣󠁴󠁿 🏴󠁧󠁢󠁷󠁬󠁳󠁿 🇺🇸 🇺🇾 🇺🇿 🇻🇺 🇻🇦 🇻🇪 🇻🇳 🇼🇫 🇪🇭 🇾🇪 🇿🇲 🇿🇼\n",
    "'''.replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma função para remover os termos que se iniciam com `#`, `@` `http`, de modo a retirar hashtags, mentions e links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(row, string_with_chars_to_remove):\n",
    "    \n",
    "    # Cria uma lista de palavras\n",
    "    words = row.text.split()\n",
    "    \n",
    "    # Remove hashtags, mentions e links\n",
    "    words = [word for word in words if ( (word[0] not in ['#', '@']) \n",
    "             and (word.startswith('http') is False)\n",
    "             and (word.startswith('www.') is False)\n",
    "             and (word.startswith('t.co') is False)\n",
    "             and (word.startswith('bit.ly') is False)\n",
    "             and (word.startswith('goo.gl') is False)\n",
    "             and (word.startswith('migre.me') is False) )\n",
    "            ]\n",
    "\n",
    "    # Remove qualquer termo que não contenha nenhum caractere do alfabeto, incluindo acentos\n",
    "    words = [word for word in words if any(letter.isalpha() for letter in word)]\n",
    "    \n",
    "    # Remove pontuação do começo e final das palavras\n",
    "    for i in range(len(words)):\n",
    "        words[i] = words[i].strip((string.punctuation + emojistring))\n",
    "    \n",
    "    # Remove emojis do meio de palavras\n",
    "    # https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    for i in range(len(words)):\n",
    "        words[i] = RE_EMOJI.sub(r'', words[i])\n",
    "        \n",
    "    text = ' '.join(words)\n",
    "     \n",
    "    return pd.Series({'text':text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplica a função\n",
    "df['text'] = df.apply(clean, args=(emojistring,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recria os dataframes de candidatos, agora para analisar, sem os caracteres especiais\n",
    "lula = df[df.name=='Lula']\n",
    "ciro = df[df.name=='Ciro Gomes']\n",
    "bolsonaro = df[df.name=='Jair Bolsonaro']\n",
    "marina = df[df.name=='Marina Silva']\n",
    "alckmin = df[df.name=='Geraldo Alckmin']\n",
    "temer = df[df.name=='Michel Temer']\n",
    "maia = df[df.name=='Rodrigo Maia']\n",
    "collor = df[df.name=='Fernando Collor']\n",
    "amoedo = df[df.name=='João Amoedo']\n",
    "boulos = df[df.name=='Guilherme Boulos']\n",
    "manuela = df[df.name==\"Manuela d'Ávila\"]\n",
    "alvaro = df[df.name==\"Álvaro Dias\"]\n",
    "wagner = df[df.name==\"Jaques Wagner\"]\n",
    "haddad = df[df.name==\"Fernando Haddad\"]\n",
    "meirelles = df[df.name==\"Henrique Meirelles\"]\n",
    "joaquim = df[df.name==\"Joaquim Barbosa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar\n",
    "Cria um segundo dataframe com os percentuais de uso de cada palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lista de palavras para ignorar usando os pacotes do NLTK\n",
    "stop_words = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cria um tokenizador usando textblob\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token for token in tokens if (len(token) >= 2)] # Pega apenas palavras maiores que três letras\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate(df, candidate, twitter_handle):\n",
    "    \n",
    "    # Filtra o dataframe para manter apenas os demais\n",
    "    others = df[df.twitter_handle!=twitter_handle] # Retirar este comentário para permitir uma comparação\n",
    "                                                    # de palavras do candidato X palavras de todos menos o candidato.\n",
    "                                                    # Com o comentário ativo, a comparação é candidatos X todos\n",
    "    # Define um vetorizador par os resultados\n",
    "    vec = CountVectorizer(tokenizer=textblob_tokenizer,\n",
    "                      stop_words=stop_words)\n",
    "    print('\"vec\" defined')\n",
    "    \n",
    "    # 'Fit' na coluna de tweets\n",
    "    matrix_candidate = vec.fit_transform(candidate.text)\n",
    "    print('\"matrix_candidate\" defined')\n",
    "\n",
    "    # E retorna um novo dataframe\n",
    "    candidate_words = pd.DataFrame(matrix_candidate.toarray(), columns=vec.get_feature_names())\n",
    "    print('\"candidate_words\" defined')\n",
    "\n",
    "    # Repete o processo para o dataframe 'other'\n",
    "    matrix_other = vec.fit_transform(others.text)\n",
    "    print('\"matrix_other\" defined')\n",
    "    other_words = pd.DataFrame(matrix_other.toarray(), columns=vec.get_feature_names())\n",
    "    print('\"other_words\" defined')\n",
    "\n",
    "    # Transpondo os arquivos para que as palavras fiquem em eixo verical\n",
    "    candidate_words =  candidate_words.transpose().reset_index() \n",
    "    other_words = other_words.transpose().reset_index()\n",
    "    print('Index reseted')\n",
    "    \n",
    "    # Diminui decimais\n",
    "    candidate_words = candidate_words.round(5)\n",
    "    other_words = other_words.round(5)\n",
    "\n",
    "    # Descobrir quantas vezes uma palavra é usada\n",
    "    candidate_words['__sum_candidate'] = candidate_words.sum(axis=1) # Os dois '_' evitam confundir com a palavra 'soma'\n",
    "    other_words['__sum_others'] = other_words.sum(axis=1) # Os dois '_' evitam confundir com a palavra 'soma'\n",
    "    print('Sum calculated')\n",
    "\n",
    "    # Descobrir o total de palavras em cada um dos dataframes\n",
    "    candidate_words['__total_candidate'] = candidate_words.__sum_candidate.sum(axis=0)\n",
    "    other_words['__total_others'] = other_words.__sum_others.sum(axis=0)\n",
    "    print('Total calculated')\n",
    "\n",
    "    # Descobrir os 'ratios' também\n",
    "    candidate_words['__ratio_candidate'] = (candidate_words['__sum_candidate'] / candidate_words['__total_candidate']) * 10000\n",
    "    other_words['__ratio_others'] = (other_words['__sum_others'] /  other_words['__total_others']) * 10000\n",
    "    print('Ratio calculated')\n",
    "\n",
    "    # Cria um único dataframe mesclando os dois. Mantém todos os termos (outer join)\n",
    "    results = candidate_words.merge(other_words, on='index', how='outer') # Note que a coluna 'index' se refere às PALAVRAS\n",
    "    print('Merge performed')\n",
    "\n",
    "    # Calcula quantas vezes é mais provável de aparecer em cada grupo\n",
    "    results[\"more_likely_candidate\"] = results.__ratio_candidate / results.__ratio_others\n",
    "    results[\"more_likely_others\"] = results.__ratio_others / results.__ratio_candidate\n",
    "    print('Likelihood calculated')\n",
    " \n",
    "    # Calcula total e ratios somados\n",
    "    results['__sum_all'] = (results.__sum_candidate + results.__sum_others)\n",
    "    results['__total_all'] = (results.__total_candidate + results.__total_others)\n",
    "    results['__ratio_all'] = (results.__sum_all / results.__total_all) * 10000\n",
    "    print(\"All ratios calculated\")\n",
    "    \n",
    "    # Mantendo apenas as colunas com informação numérica\n",
    "    results = results[['index','__sum_candidate','__sum_others',\n",
    "                       '__total_candidate','__total_others',\n",
    "                       '__ratio_candidate', '__ratio_others',\n",
    "                       'more_likely_candidate', 'more_likely_others',\n",
    "                       '__sum_all', '__total_all', '__ratio_all']]\n",
    "    print('Columns filtered')\n",
    "    \n",
    "    \n",
    "    # Muda o cabeçalho de 'index' para 'word'\n",
    "    results.columns = ['word','__sum_candidate','__sum_others',\n",
    "                       '__total_candidate','__total_others',\n",
    "                       '__ratio_candidate', '__ratio_others',\n",
    "                       'more_likely_candidate', 'more_likely_others',\n",
    "                       '__sum_all', '__total_all', '__ratio_all']\n",
    "    print('Columns renamed')\n",
    "\n",
    "    # Cria uma coluna com identificador\n",
    "    results['candidate'] = twitter_handle\n",
    "    \n",
    "    # Arredonda\n",
    "    results = results.round(decimals=5)\n",
    "\n",
    "    # Salva os resultados que contenham nan para a soma de palavras dos outros, \n",
    "    # ou seja, salva as palavras usadas apenas pelo candidato em questão\n",
    "    results_unique = results[results['__sum_others'].isnull()]\n",
    "\n",
    "    # Derruba palavras não usadas pelo candidato, que terão um NaN no meio.\n",
    "    results = results.dropna()\n",
    "    \n",
    "\n",
    "    return results, results_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defina uma função para salvar\n",
    "def save_csv(k, v, directory):\n",
    "    path = directory + k + '-matrix.csv'   \n",
    "    v.to_csv(path, index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lulapelobrasil:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "cirogomes:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "jairbolsonaro:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "silva_marina:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "geraldoalckmin:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "MichelTemer:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "RodrigoMaia:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "collor:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "joaoamoedonovo:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "GuilhermeBoulos:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "ManuelaDavila:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "alvarodias_:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "Haddad_Fernando:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "jaqueswagner:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "meirelles:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "joaquimboficial:\n",
      "\"vec\" defined\n",
      "\"matrix_candidate\" defined\n",
      "\"candidate_words\" defined\n",
      "\"matrix_other\" defined\n",
      "\"other_words\" defined\n",
      "Index reseted\n",
      "Sum calculated\n",
      "Total calculated\n",
      "Ratio calculated\n",
      "Merge performed\n",
      "Likelihood calculated\n",
      "All ratios calculated\n",
      "Columns filtered\n",
      "Columns renamed\n",
      "\n",
      "CPU times: user 23min 9s, sys: 4min 23s, total: 27min 32s\n",
      "Wall time: 27min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Aplica a análise para cada candidato, salvando os resultados em um dicionário\n",
    "results = {}\n",
    "dfs = [lula, ciro, bolsonaro, marina, \n",
    "       alckmin, temer, maia, collor, amoedo, \n",
    "       boulos, manuela, alvaro, haddad, wagner,\n",
    "       meirelles, joaquim]\n",
    "for candidate_df, twitter_handle in zip(dfs, ['lulapelobrasil', 'cirogomes', 'jairbolsonaro', 'silva_marina', \n",
    "                                          'geraldoalckmin', 'MichelTemer', 'RodrigoMaia', 'collor', 'joaoamoedonovo',\n",
    "                                          'GuilhermeBoulos', 'ManuelaDavila',  'alvarodias_',\n",
    "                                          'Haddad_Fernando','jaqueswagner', 'meirelles', 'joaquimboficial']):\n",
    "    print(twitter_handle + ':')\n",
    "    results[twitter_handle], results_unique[twitter_handle] = calculate(df, candidate_df, twitter_handle)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Salva cada item do dicionário em um csv separado\n",
    "for k,v in results.items():\n",
    "    save_csv(k, v, '../data/matrix-geral/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
